{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdbLogisticRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUx3q17CYvFoT66NynIV5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushasgorawar/ML/blob/main/imdbLogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7gzoxHbtwmAK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "wnet = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7fC9C-wvZm",
        "outputId": "7d86ddff-4b27-4037-f543-a6ab25e882b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeXt3dHjwzkb",
        "outputId": "535a79ea-5825-461c-c461-34d05c5f2953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/kaggle_dataset'\n",
        "%cd /content/gdrive/MyDrive/kaggle_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feNiYBq1w5xe",
        "outputId": "20aed2fa-19c5-484b-dc49-6189a2c48d17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/kaggle_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"IMDB_Dataset.csv\"\n",
        "df = pd.read_csv(url)\n",
        "#,encoding='ISO-8859-1'\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZ-eZWHw9GT",
        "outputId": "6234f145-a87e-48bd-f672-a97c6f8ff11b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['sentiment']\n",
        "df[cols] = df[cols].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "X = df.iloc[:, 0].values\n",
        "y = df.iloc[:,1].values\n"
      ],
      "metadata": {
        "id": "7zIBZVG3w_Cg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = [\"a\", \"about\", \"actually\", \"almost\", \"also\", \"although\", \"always\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
        "      \"be\", \"became\", \"become\", \"but\", \"by\", \"can\", \"could\", \"did\", \"do\", \"does\", \"each\", \"either\", \"else\", \"for\",\n",
        "      \"from\", \"had\", \"has\", \"have\", \"hence\", \"how\", \"i\", \"if\", \"in\", \"is\", \"it\", \"its\", \"just\", \"may\",\"many\", \"maybe\", \"me\",\n",
        "      \"might\", \"mine\", \"must\", \"my\", \"mine\", \"must\", \"my\", \"of\", \"oh\", \"ok\", \"then\",\"there\",\"this\",\"that\" \"was\", \"when\", \"where\", \"whereas\", \"wherever\",\n",
        "      \"whenever\", \"whether\", \"which\", \"while\", \"who\", \"whom\", \"whoever\", \"whose\", \"why\", \"will\", \"with\", \"within\",\n",
        "      \"without\", \"would\", \"yes\", \"yet\", \"you\", \"your\", \"the\",\"here\",\"i\",\"am\",\"too\"]"
      ],
      "metadata": {
        "id": "xqv3jNoYxOdg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\n",
        "    \"ain't\": \"am not / are not\",\n",
        "    \"aren't\": \"are not / am not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he had / he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he shall / he will\",\n",
        "    \"he'll've\": \"he shall have / he will have\",\n",
        "    \"he's\": \"he has / he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how has / how is\",\n",
        "    \"i'd\": \"I had / I would\",\n",
        "    \"i'd've\": \"I would have\",\n",
        "    \"i'll\": \"I shall / I will\",\n",
        "    \"i'll've\": \"I shall have / I will have\",\n",
        "    \"i'm\": \"I am\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it had / it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it shall / it will\",\n",
        "    \"it'll've\": \"it shall have / it will have\",\n",
        "    \"it's\": \"it has / it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she had / she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she shall / she will\",\n",
        "    \"she'll've\": \"she shall have / she will have\",\n",
        "    \"she's\": \"she has / she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so as / so is\",\n",
        "    \"that'd\": \"that would / that had\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that has / that is\",\n",
        "    \"there'd\": \"there had / there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there has / there is\",\n",
        "    \"they'd\": \"they had / they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they shall / they will\",\n",
        "    \"they'll've\": \"they shall have / they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we had / we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what shall / what will\",\n",
        "    \"what'll've\": \"what shall have / what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what has / what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when has / when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where has / where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who shall / who will\",\n",
        "    \"who'll've\": \"who shall have / who will have\",\n",
        "    \"who's\": \"who has / who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why has / why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you had / you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you shall / you will\",\n",
        "    \"you'll've\": \"you shall have / you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "0SpU7qoOxRdf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def getcleanedText(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('<br />','')\n",
        "    text = text.replace('\\\\','')\n",
        "\n",
        "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z\\t])\", \" \", text).split())\n",
        "    for word in text.split():\n",
        "      if word.lower() in contractions:\n",
        "        text = text.replace(word, y[word.lower()])\n",
        "    # tokenize\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    new_tokens = [token for token in tokens if token not in sw]\n",
        "\n",
        "    stemmed_tokens = [wnet.lemmatize(tokens) for tokens in new_tokens]\n",
        "    clean_text = \" \".join(stemmed_tokens)\n",
        "\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "IA2axUHWxSp4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_clean = [getcleanedText(i) for i in X]       "
      ],
      "metadata": {
        "id": "JBQ3V8ijAVF3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer            \n",
        "                                                                       \n",
        "vectorizer = TfidfVectorizer(min_df=2)                                 \n",
        "tfidf = vectorizer.fit_transform(X_clean)                              "
      ],
      "metadata": {
        "id": "s1xBFi4QxY8J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidf, y, test_size=0.3, random_state=0)  "
      ],
      "metadata": {
        "id": "scjDHOOnxbMA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression                  \n",
        "                                                                     \n",
        "model = LogisticRegression().fit(X_train, y_train)                   \n",
        "score = model.predict_proba(X_test)                                  "
      ],
      "metadata": {
        "id": "8O6J0iuDxisK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [int(p[1] > 0.5) for p in score]         \n",
        "                                                  \n",
        "acc=np.sum(y_pred == y_test)/len(y_pred)          \n",
        "print(\"ACCURACY = \")                              \n",
        "print(acc*100)                                    "
      ],
      "metadata": {
        "id": "pvjXsQDkxpuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c50fe8a-57c1-4623-bb0f-dd0c9874492a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY = \n",
            "89.42666666666666\n"
          ]
        }
      ]
    }
  ]
}